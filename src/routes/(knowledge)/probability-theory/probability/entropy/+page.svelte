<script>
    import Title from "$lib/Elements/Title.svelte";
    import Theorem from "$lib/Stencils/Blocks/Theorem.svelte";
    import Math from "$lib/Math/Math.svelte";
    import P from "$lib/Markup/P.svelte";
    import Par from "$lib/Markup/Par.svelte";
    import Subheading from "$lib/Markup/Subheading.svelte";
    import Fact from "$lib/Stencils/Blocks/Fact.svelte";
    import Heading from "$lib/Markup/Heading.svelte";
</script>

<Title title="Энтропия"
/>

<Par>
    <P>
        Речь пойдет преимущественно про разные системы. С точки зрения вероятностной интерпретации происходящего, можно
        рассматривать систему как совокупность состояний, каждое из которых может реализовываться с какой-то
        вероятностью. Тогда событие в этом вероятностном пространстве представляют собой реализацию какого-то состояния
        из множества.
    </P>
</Par>

<Heading>Энтропия</Heading>

<Par>
    <P>
        <b>Энтропия</b>&nbsp;&mdash; мера неопределенности или мера хаоса какой-то системы.
        Обозначается буквой <Math m="\H"/>.
    </P>
    <P>
        Чем больше неопределенность системы, тем больше нужно информации для кодирования её состояния. Получается, что
        энтропия служит еще и мерой информации, необходимой для описания состояния системы.
    </P>
</Par>

<Par>
    <P>Можно определить <b>энтропию Шеннона</b> как</P>
    <Math display
          m={`\\H(\\Omega) \\defeq - \\sum_{\\omega \\in \\Omega} \\prob(\\omega) \\cdot \\log \\prob(\\omega)`}/>
    <P>Для непрерывного случая с носителем <Math m="X"/> и плотностью распределения <Math m="f(x)"/></P>
    <Math display m={`\\H \\defeq - \\int_{X} f(x) \\cdot \\log f(x) dx`}/>
</Par>

<Fact title="Откуда взялась эта формула?">
    <P>
        Давайте попробуем оценить количество информации,
        необходимое для кодирования одного исхода.
    </P>
    <P>
        Посмотрим на&nbsp;<Math m="n"/> испытаний, среди которых <Math m="a \approx pn"/> удачных
        и&nbsp;<Math m="b \approx (1-p) \cdot n"/> неудачных. Результаты испытаний для
        известных <Math m="a"/> и <Math m="b"/> кодируются последовательностью бит.
        Всего таких последовательностей <Math m={`\\binom{n}{a} = \\frac{n!}{a! \\cdot b!}`}/>.
        Значит, для кодирования каждой последовательности достаточно <Math m={`\\log \\frac{n!}{a! \\cdot b!}`}/> бит.
    </P>
    <P>
        Получается, что в среднем для кодирования результатов одного исхода
        понадобится в среднем <Math m={`\\frac{1}{n} \\log \\frac{n!}{a! \\cdot b!}`}/> бит.
        По&nbsp;формуле Стирлинга <Math m="\log n! = n \log n - n + O(\log n)"/>,
    </P>
    <Math display m={`\\frac{1}{n} \\cdot \\log \\frac{n!}{a! \\cdot b!} =
                      \\frac{1}{n} \\cdot \\big(\\log n! - \\log a! - \\log b!\\big) \\approx
                      - p \\log p - (1-p) \\cdot \\log (1-p)`}/>
</Fact>

<Par>
    <P>
        Например, у системы &laquo;брошенная монетка&raquo; с распределением
        вероятностей <Math m={`\\big(\\frac{1}{2}, \\frac{1}{2}\\big)`}/>, энтропия
    </P>
    <Math display m={`\\H = 2 \\cdot \\frac{1}{2} \\cdot \\log_2 2 = 1`}/>
    <P>
        А у системы &laquo;брошенная ненормальная монетка&raquo; с распределением
        вероятностей <Math m={`\\big(\\frac{1}{4}, \\frac{3}{4}\\big)`}/> энтропия
    </P>
    <Math display m={`\\H = \\frac{1}{4} \\cdot \\log_2 4 + \\frac{3}{4} \\cdot \\log_2 \\frac{4}{3} \\approx 0.56`}/>
    <P>
        Энтропия нормальной монетки больше энтропии ненормальной. Значит, сообщение о результате броска нормальной
        монетки несет больше информации, чем сообщение о результате броска ненормальной.
    </P>
</Par>


<Subheading>Свойства энтропии</Subheading>

<Par>
    <P>Энтропия системы равна нулю только тогда, когда система состоит из одного состояния.</P>
    <P>Действительно, сообщение о состоянии такой системы не несет вообще никакой информации. Мы и так это знали.</P>
</Par>

<Par>
    <P>Энтропия системы с <Math m="n"/> состояниями ограничена числом <Math m="\log n"/>.</P>
    <Math display
          m={`\\H = \\sum_{i=1}^n p_i \\log \\frac{1}{p_i} \\le \\log \\bigg( \\sum_{i=1}^n p_i \\cdot \\frac{1}{p_i} \\bigg) = \\log n`}/>
</Par>

<Par>
    <P>Если <Math m="X"/> и <Math m="Y"/>&nbsp;&mdash; независимые системы, то <Math m="\H(X \cdot Y) = \H(X) + \H(Y)"/>.
    </P>
</Par>

<Subheading>Примеры распределений</Subheading>

<Par>
    <P>
        Энтропия равномерного распределения <Math m="\uniform [a, b]"/>
    </P>
    <Math display m={`\\H = \\int_a^b \\frac{1}{b-a} \\cdot \\log (b-a) dx = \\log (b-a)`}/>
</Par>

<Par>
    <P>
        Энтропия нормального распределения <Math m="\N (\mu, \sigma^2)"/> с
        плотностью <Math m={`f(x) = \\frac{1}{\\sqrt{2\\pi} \\sigma} e^{- \\frac{(x-\\mu)^2}{2\\sigma^2}}`}/>
    </P>
    <Math display m={`\\H =
                      - \\int_{-\\infty}^{\\infty}
                        \\frac{1}{\\sqrt{2\\pi} \\sigma} e^{- \\frac{(x-\\mu)^2}{2\\sigma^2}} \\cdot
                        \\log \\bigg( \\frac{1}{\\sqrt{2\\pi} \\sigma} e^{- \\frac{(x-\\mu)^2}{2\\sigma^2}} \\bigg) dx =
                      \\frac{1}{2} \\log (2 \\pi \\sigma^2) + \\frac{1}{2}`}/>
</Par>


<Par>
    <P>
        Энтропия показательного распределения <Math m="\Exp (\lambda)"/> с
        плотностью <Math m={`f(x) = \\lambda e^{-\\lambda x}`}/>
    </P>
    <Math display m={`\\H = - \\int_{0}^{\\infty}
                              \\lambda e^{-\\lambda x} \\cdot \\log \\big( \\lambda e^{-\\lambda x} \\big) dx =
                              1 - \\log \\lambda`}/>
</Par>


<Heading>Расстояние Кульбака-Лейблера и кросс-энтропия</Heading>

<Par>
    <P>
        Пусть у нас есть две системы <Math m="p"/> и <Math m="q"/>.
        Как понять, насколько <Math m="q"/> отличается от <Math m="p"/>?
    </P>
</Par>

<Subheading>Кросс-энтропия</Subheading>

<Par>
    <P>
        <b>Кросс-энтропия</b> служит показателем информации, необходимой для распознания одного исхода, если схема
        кодирования базируется не на истинном распределении <Math m="p"/>, а на другом распределении <Math m="q"/>.
    </P>
    <P>
        Эту сложную фразу можно записать как
    </P>
    <Math display m={`\\H(p, q) \\defeq - \\sum_x p(x) \\cdot \\log q(x) = - \\int_X p(x) \\cdot \\log q(x) dx`}/>
</Par>

<Par>
    <P>
        Если распределение <Math m="q"/> близко к распределению <Math m="p"/>, то кросс-энтропия <Math m="\H(p, q)"/>
        близка к обычной энтропии <Math m="\H(p)"/>, а полное совпадение <Math m="\H(p, q) = \H(p)"/> происходит в
        случае, когда распределения <Math m="p"/> и <Math m="q"/> совпадают почти всюду.
    </P>
</Par>

<Subheading>Расстояние Кульбака-Лейблера</Subheading>

<Par>
    <P>Теперь мы можем измерять степень различия двух распределений.</P>
    <P>
        <b>Расстоянием Кульбака-Лейблера</b> между двумя распределениями <Math m="p"/> и <Math m="q"/> называется
        величина
    </P>
    <Math display m={`\\KL(p, q) \\defeq \\H(p, q) - \\H(p) = - \\sum_x p(x) \\cdot \\log \\frac{p(x)}{q(x)}`}/>
    <P>
        Расстояние Кульбака-Лейблера говорит об увеличении среднего количества информации, если при кодировании
        использовать распределение <Math m="q"/> вместо истинного распределения <Math m="p"/>.
    </P>
</Par>

<Theorem title="Принцип максимальной энтропии">
    <P>Среди всех распределений на заданном носителе мы хотим иметь дело с имеющим наибольшую энтропию.</P>
    <P>Довольно естественно: чем больше энтропия, тем более &laquo;произвольное&raquo; у нас распределение.</P>
</Theorem>

