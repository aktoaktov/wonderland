<script>
    import Title from "$lib/Elements/Title.svelte"
    import Par from "$lib/Markup/Par.svelte"
    import P from "$lib/Markup/P.svelte"
    import Math from "$lib/Math/Math.svelte"
    import Codeblock from "$lib/Code/Codeblock.svelte"
    import Heading from "$lib/Markup/Heading.svelte"
    import Subheading from "$lib/Markup/Subheading.svelte"
</script>

<Title title="Последовательный поиск"
/>

<Par>
    <P>
        Последовательный поиск &mdash; поиск в конечном итераторе.
        Сразу же с порога коллизия в терминологии.
        Здесь важно, что мы можем двигаться только вперед (в некоторых экзотических случаях и назад),
        но произвольного доступа у нас нет. То есть мы не можем вытащить элемент с конкретным номером.
    </P>
</Par>

<Par>
    <P>
        Пусть у нас есть итератор из ключей <Math m={`K_1, K_2, \\dotsc, K_n`}/> (на их структуру нам пока наплевать).
        Нам нужно понять, есть ли в этом наборе ключ <Math m={`t`}/>, и, если он есть, найти его номер.
    </P>
    <P>
        Банальная идея &laquo;начать с начала и искать, пока не найдешь&raquo; не так уж и плоха.
        Код, реализующий этот подход
    </P>
</Par>

<Codeblock code={
`function search(iterable x, target) -> bool:
    for index, element of enumerate(x):
        if element == target:
            return index
    return not found`}
/>

<Par>
    <P>
        Этот же алгоритм применим и для массивов (в общем случае последовательностей),
        про внутреннюю структуру которых мы ничего не знаем.
        Более того, при отсутствующей информации о структуре это самый эффективный алгоритм.
    </P>
</Par>


<Par>
    <P>
        Понятно, что такой алгоритм работает за время <Math m={`O(n)`}/>.
    </P>
    <P>
        Показатель времени работа такого алгоритма &mdash; количество проверок (сравнений) <Math m={`C`}/>.
        Если позиция целевого элемента равновероятно может равняться любому числу от <Math m={`1`}/> до <Math m={`n`}/>,
        то ожидаемое количество проверок
    </P>
    <Math display
          m={`\\expect C = \\sum_{i=1}^n i \\cdot \\prob(i) = \\frac{1}{n} \\sum_{i=1}^n i = \\frac{n+1}{2}`}/>
</Par>


<Heading>Упорядоченная последовательность</Heading>

<Par>
    <P>
        Пусть все элементы нашего итератора упорядочены, то есть <Math m={`x_j > x_i`}/> для <Math m={`j > i`}/>.
    </P>
    <P>
        Мы хотим найти элемент <Math m={`t`}/> и его позицию.
        Если на какой-то итерации <Math m={`i`}/> элемент <Math m={`x_i`}/> оказался больше <Math m={`t`}/>,
        то дальше продолжать смысла нет: из-за упорядоченности итератора все остальные
    </P>
</Par>

<Codeblock code={
`function search(iterable x, target):
    for index, element of enumerate(x):
        if element == target:
            return index
        if element > target:
            return not found

    return not found`}
/>

<Par>
    <P>
        Временная сложность этого алгоритма совпадает с временной сложностью обычного алгоритма
        последовательного поиска.
        Отличие здесь состоит лишь в том, что для отсутствующих в итераторе элементов этот факт отсутствия может быть
        установлен гораздо раньше.
    </P>
</Par>


<Heading>Частота обращений</Heading>

<Par>
    <P>
        Мы предполагали, что все аргументы последовательного поиска равновероятны.
        Вообще это не так. Распределение вероятностей может быть любым.
    </P>
    <P>
        Пусть вероятность запроса на поиск ключа <Math m={`K_i`}/> равна <Math m={`p_i`}/>.
        И пусть ответ на наш запрос всегда есть, то есть искомый элемент точно в итераторе.
    </P>
    <Math display m={`\\sum_{i=1}^n p_i = 1`}/>

    <P>
        Математическое ожидание количества проверок в алгоритме последовательного поиска равно
    </P>
    <Math display m={`\\expect C = \\sum_{i=1}^n i \\cdot p_i`}/>
</Par>

<Par>
    <P>
        Если нам подвластна структура итератора, то минимальное ожидаемое время работы достигается при
    </P>
    <Math display m={`p_1 \\ge p_2 \\ge \\dotsb \\ge p_n`}/>.
    <P>
        Вообще, это все немного противоречит самой концепции итераторов,
        но допустим, что мы с самого начала определяем структуру итератора, а потом этим пользуемся.
    </P>
</Par>


<Subheading>Распределение Зипфа</Subheading>

<Par>
    <P>
        Более типично для реальных ситуаций распределение Зипфа:
    </P>
    <Math display m={`p_1 = \\frac{1}{H_n} \\quad p_2 = \\frac{1}{2 H_n} \\quad \\cdots \\quad
                      p_i = \\frac{1}{i H_n} \\quad \\cdots \\quad p_n = \\frac{1}{n H_n}`}/>

    <P>
        Джордж Кингсли Зипф (George Kingsley Zipf), изучая статистические закономерности в естественных языках,
        обнаружил, что <Math m={`n`}/>-е по частоте слово встречается с частотой,
        примерно обратно пропорциональной <Math m={`n`}/>.
    </P>
    <P>
        По этому распределению <Math m={`p_i = c/i`}/>.
        Осталось найти константу из условия нормировки
    </P>
    <Math display m={`\\sum_{i=1}^n p_i = 1 \\implies c \\cdot H_n = 1 \\implies c = \\frac{1}{H_n}`}/>

    <P>
        При таком распределении ожидаемое количество сравнений
    </P>
    <Math display m={`\\expect C = \\frac{n}{H_n} \\sim \\frac{n}{\\ln n}`}/>
    <P>
        Получается, что это почти в <Math m={`\\frac{1}{2} \\ln n`}/> раз быстрее,
        чем при предположении о равномерном распределении.
    </P>
</Par>


<Subheading>Распределение Парето</Subheading>

<Par>
    <P>
        Еще более близкое к реальному распределение Парето (наиболее известное как правило 80-20),
        которое на самом деле возникло в процессе изучения спроса и транзакций.
        Человеческим языком его можно сформулировать так &laquo;80% транзакций работают с 20% данных&raquo;.
    </P>
    <P>
        Коротко, можно записать это распределение (с упорядоченными вероятностями) формулой
    </P>
    <Math display m={`\\frac{p_1 + p_2 + \\dotsb + p_{0.2 \\cdot m}}{p_1 + p_2 + \\dotsb + p_m} = 0.8`}/>
    <P>
        На практике, значения 20% и 80% могут чуть-чуть отличатся, поэтому целесообразно ввести коэффициент
    </P>
    <Math display m={`\\theta = \\frac{\\log 0.8}{\\log 0.2}`}/>
</Par>

<Par>
    <P>
        Из формулы-определения следуют формулы для вероятностей в терминах коэффициента <Math m={`\\theta`}/>
    </P>
    <Math display m={`p_1 = \\frac{1}{n^\\theta} \\quad p_2 = \\frac{2^\\theta - 1}{n^\\theta} \\quad \\cdots \\quad
                      p_i = \\frac{i^\\theta - (i-1)^\\theta}{n^\\theta} \\quad \\cdots \\quad p_n = \\frac{n^\\theta - (n-1)^\\theta}{n^\\theta}`}/>

    <P>
        Теперь можно найти ожидаемое количество сравнений
    </P>
    <Math display m={`\\expect C = \\sum_{i=1}^n i \\cdot \\frac{i^\\theta - (i-1)^\\theta}{n^\\theta} =
    \\frac{1}{n^\\theta} \\sum_{i=1}^n i \\cdot \\big( i^\\theta - (i-1)^\\theta \\big) =
    n + 1 - \\frac{1}{n^\\theta} \\sum_{i=1}^n i^\\theta`}/>

    <P>
        Используя асимптотическую оценку суммы
        <Math m={`\\sum_{i=1}^n i^\\theta \\sim \\frac{n^{\\theta+1}}{\\theta+1} + \\frac{n^\\theta}{2} + O(n^{\\theta-1})`}/>
        получаем, что
    </P>
    <Math display m={`\\expect C = n + 1 - \\frac{1}{n^\\theta} \\sum_{i=1}^n i^\\theta =
     \\frac{\\theta}{\\theta+1} \\cdot n + \\frac{1}{2} + O \\left( \\frac{1}{n} \\right)`}/>
</Par>

<Par>
    <P>
        Результат довольно хороший.
        При коэффициенте <Math m={`\\theta = \\log_{0.2}{0.8} \\approx 0.139`}/> количество сравнений оценивается
        по только что выведенной формуле как <Math m={`0.5 + 0.122037 \\cdot n`}/>.
    </P>
    <P>
        Несмотря на немного разную грубость в оценках по Зипфу и по Паретто, распределение Паретто
        выигрывает у распределение Зипфа примерно до значения <Math m={`n = 3586`}/>.
    </P>
</Par>